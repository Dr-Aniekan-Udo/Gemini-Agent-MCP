# import libraries
import os

# import async libraries and  mcp client component
from typing import Optional
from aioconsole import ainput  # Async input library
from contextlib import AsyncExitStack
from mcp import ClientSession, StdioServerParameters  # for managing mcp sessions
from mcp.client.stdio import stdio_client  # mcp component for standard I/O

# import Gemini AI SDK
from google import genai
from google.genai import types
# from google.genai.types import GenerateContentConfig

# import enviroment loader
from dotenv import load_dotenv

# import utility script
from gemini_utilities.tools_converter import convert_mcp_tools_to_gemini
from gemini_utilities.system_input import system_instruction, tool_instruction

# load env
dotenv_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), '.env')
load_dotenv(dotenv_path=dotenv_path)

# server path
current_file_path = __file__  # Path to stdio_chat_loop.py
current_dir = os.path.dirname(current_file_path)
server_file_path = os.path.join(current_dir, "..", "ga4_server.py")


# build client
class MCPClient:
    def __init__(self):
        """ initialize the mcp client and configure the GEmimi API"""
        # mcp session for interaction
        self.session: Optional[ClientSession] = None
        # manage asyn cleanup
        self.exit_stack = AsyncExitStack()
        # To store conversation context
        self.conversation_history = []
        # get Gemini APi key from env
        gemini_api_key = os.getenv("GEMINI_API_KEY")
        if not gemini_api_key:
            raise ValueError("GEMINI_API_KEY not found, please add it to your .env file")
        # configure the Gemini AI client
        self.genai_client = genai.Client(api_key=gemini_api_key)
    
    async def connect_to_server(self, server_file_path: str):
        """connect to mcp server an list available tools

        Args:
            server_file_path (str): takes in the file path to server script
        """
        # determine the script language
        command = "python" if server_file_path.endswith(".py") else "node"
        # define the parameter for connecting the MCP server
        # check commands
        print(f"Command: {command}, Args: {server_file_path}")

        server_params = StdioServerParameters(command=command, args=[server_file_path])
        # establish connection with MCP server
        stdio_transport = await self.exit_stack.enter_async_context(
            stdio_client(server_params)
            )
        # extract the read and write transport object
        self.stdio, self.write = stdio_transport
        # initilize the mcp client
        self.session = await self.exit_stack.enter_async_context(
            ClientSession(self.stdio, self.write)
            )
        # send initilization request to the server
        await self.session.initialize()
        # request the list of available tools
        response = await self.session.list_tools()
        tools = response.tools  # extract the list of tools form  the response
        # print the name of the tools available in the server
        print("\nConnected to server with tools: ", [tool.name for tool in tools])
        # convert mcp tools to Gemini format and attache it to the class
        self.function_declaration = convert_mcp_tools_to_gemini(tools)

    async def process_query(self, query: str):
        """process the user query using the geimin API and execute tool calls if needed

        Args:
            query (str): The user input query

        Returns:
            str: The respose generated by the Gemini model
        """
        # format user input as a structured content
        self.conversation_history.append({"role": "user", "content": query})

        # Convert the conversation history into Gemini-compatible structured content

        history_content = [
            types.Content(
                # indicate message source
                role=entry["role"],
                # convert text to gemini compatible format
                parts=[types.Part.from_text(text=entry["content"])]
            )
            for entry in self.conversation_history
        ]

        # send user input to Gemini AI along with the available tools
        response = self.genai_client.models.generate_content(
            # specify the model to use
            model="gemini-2.0-flash-001",
            # add user prompt
            contents=history_content,  # Include the entire history as context

            # pass the tools
            config=types.GenerateContentConfig(
                tools=self.function_declaration,
                system_instruction=tool_instruction
                )
        )
        # initialize variable to store final resources
        # store text
        final_text = []
        # store assistance message
        # assistant_message_context = []
        # process the response recieved from Gemini
        for candidate in response.candidates:
            # ensure respond as content
            if candidate.content.parts:
                # check if part is a valid gemini response unit
                for part in candidate.content.parts:
                    if isinstance(part, types.Part):
                        # check if part has function call
                        if part.function_call:
                            # extract the function call
                            function_call_part = part
                            # get the name of the mcp tool gemini wants to call
                            tool_name = function_call_part.function_call.name
                            # get thhe arguments gemini wants to pass
                            tool_args = function_call_part.function_call.args
                            # print debug information which tool is being call and with hwta arguments
                            print(f"\n[Gemini requested toolcall:{tool_name} with args{tool_args}]")

                            # execute tool with MCP server
                            try:
                                result = await self.session.call_tool(tool_name, arguments=tool_args)
                                # store the output
                                function_response = {"result": result.content}
                            except Exception as e:
                                function_response = {"error": str(e)}
                            
                            # format the tool response for gemini compatibility
                            function_response_part = types.Part.from_function_response(
                                # name of the tool
                                name=tool_name,
                                # response gotten
                                response=function_response
                            )

                            # 
                            function_response_content = types.Content(
                                role="tool",
                                parts=[function_response_part]
                            )
                            print(f"Tool response :{function_response}")

                            # send the response and original prompt to the model
                            response = self.genai_client.models.generate_content(
                                model="gemini-2.0-flash-001",
                                contents=[
                                    # attach original query
                                    history_content,
                                    # include function call tool name and args
                                    function_call_part,
                                    # include tool excution result
                                    function_response_content,
                                ],
                                config=types.GenerateContentConfig(
                                    # provide available tool for the next phase
                                    tools=self.function_declaration,
                                    response_mime_type="application/json",
                                    system_instruction=system_instruction
                                ),
                            )
                            # extract response from Gemini after processing tool call
                            final_text.append(response.candidates[0].content.parts[0].text)
                            # Add the assistant's response to the conversation history
                            self.conversation_history.append({"role": "assistant", "content": part.text})
                        else:
                            # if no tool call was requested, simply add gemini text
                            final_text.append(part.text)
                            # Add the assistant's response to the conversation history
                            self.conversation_history.append({"role": "assistant", "content": part.text})
                        # return the combined resonse as a single formatted string separated by a newline
                        return "\n".join(final_text)
    
    async def chat_loop(self):
        """run  interaction chat session with user in the loop
        """

        while True:
            print("\nMCP client started! Type 'quit' to exit, or 'clear' to reset the conversation.")
            query = await ainput("\nQuery: ")  #.strip()
            if query.lower() == 'quit':
                break
            elif query.lower() == 'clear':
                self.conversation_history = []  # Clear conversation history
                print("Conversation history cleared.")
                continue
            # processs the user's query
            response = await self.process_query(query)
            print("\nGemini: " + response)
    
    async def clean_up(self):
        """Clean the resources before exiting
        """
        await self.exit_stack.aclose()
